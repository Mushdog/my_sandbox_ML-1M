{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3790ebc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Process results\n",
    "\n",
    "Currently this just means computing metric increases,\n",
    "in both additive units and percent units.\n",
    "\n",
    "The reason this isn't non-trivial is because there are a lot of different experiments\n",
    "and it is important to match the experimental results (i.e. some boycott) to\n",
    "the standard results (i.e. performance from the perspective of the would-be boycotters in the no boycott condition)\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "import argparse\n",
    "import json\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from utils import concat_output_filename, extract_from_filename\n",
    "from plot import plot_all\n",
    "\n",
    "from constants import MEASURES, get_metric_names\n",
    "\n",
    "\n",
    "ALGO_NAMES = [\n",
    "    'SVD',\n",
    "    #'KNNBasic_user_msd'    \n",
    "    #'KNNBaseline_item_msd',\n",
    "]\n",
    "\n",
    "def main(args):\n",
    "    metric_names = get_metric_names()\n",
    "    algo_to_st = {}\n",
    "    algo_to_van = {}\n",
    "    outnames = []\n",
    "    if args.outnames:\n",
    "        outnames = args.outnames\n",
    "        print('outnames', outnames)\n",
    "    else:\n",
    "        for userfrac in args.userfracs:\n",
    "            for ratingfrac in args.ratingfracs:\n",
    "                for sample_size in args.sample_sizes:\n",
    "                    outname = concat_output_filename(\n",
    "                        args.dataset, args.grouping,\n",
    "                        userfrac,\n",
    "                        ratingfrac,\n",
    "                        sample_size, args.num_samples\n",
    "                    )\n",
    "                    outnames.append(outname)\n",
    "    for outname in outnames:\n",
    "        print(outname)\n",
    "        try:\n",
    "            userfrac = extract_from_filename(outname, 'userfrac-', 3)\n",
    "            ratingfrac = extract_from_filename(outname, 'ratingfrac-', 3)\n",
    "            experiment_type = extract_from_filename(outname, 'type-', None, '_userfrac')\n",
    "            if 'indices' in outname:\n",
    "                indices = extract_from_filename(outname, 'indices-', None, '.csv')\n",
    "        except:\n",
    "            continue\n",
    "        try:\n",
    "            err_df = pd.read_csv(outname)\n",
    "        except FileNotFoundError:\n",
    "            print('FileNotFoundError')\n",
    "            continue\n",
    "        err_df = err_df.set_index('Unnamed: 0')\n",
    "        uid_to_metric = err_df.to_dict(orient='index')\n",
    "\n",
    "        # this is dangerous. a lot of issues w/ accidental variable assignment\n",
    "        for algo_name in ALGO_NAMES:\n",
    "            if args.load_standards_from_json:\n",
    "                standards_filename = 'MERGED_{}_{}.json'.format(args.dataset, algo_name)\n",
    "                if algo_to_st.get(algo_name) is None:\n",
    "                    try:\n",
    "                        with open(standards_filename, 'r') as f:\n",
    "                            algo_to_st[algo_name] = json.load(f)\n",
    "                    except:\n",
    "                        print('Could not load file {} for algo {}. Moving to next algorithm'.format(\n",
    "                            standards_filename, algo_name\n",
    "                        ))\n",
    "                        continue\n",
    "                    vanilla_filename = 'standard_results/{}_ratingcv_standards_for_{}.json'.format(args.dataset, algo_name)\n",
    "                    try:\n",
    "                        with open(vanilla_filename, 'r') as f:\n",
    "                            algo_to_van[algo_name] = json.load(f)\n",
    "                    except:\n",
    "                        print('no vanilla')\n",
    "                        algo_to_van[algo_name] = {}\n",
    "            for uid, res in uid_to_metric.items():\n",
    "                if res['algo_name'] != algo_name:\n",
    "                    continue\n",
    "                for metric in metric_names:\n",
    "                    for group in ['all', 'non-boycott', 'boycott', 'like-boycott', 'all-like-boycott']:\n",
    "                        key = '{}_{}'.format(metric, group)\n",
    "                        if args.load_standards_from_json:\n",
    "                            standard_val_key = '{metric}_{group}__{outname}__{identifier}'.format(**{\n",
    "                                'metric': metric,\n",
    "                                'group': group,\n",
    "                                'outname': outname.replace('results/', ''),\n",
    "                                'identifier': uid[:4], # the first 4 characters of the uid are a 4 digit identifier #\n",
    "                            })\n",
    "                            standard_val = algo_to_st[algo_name].get(standard_val_key)\n",
    "                        else:\n",
    "                            standard_key = 'standards_' + key\n",
    "                            standard_val = res.get(standard_key)\n",
    "                        \n",
    "                        if standard_val is None:\n",
    "                            # print('No standard val for key {} in file {}'.format(\n",
    "                            #     standard_key, outname\n",
    "                            # ))\n",
    "                            pass\n",
    "                        # vanilla_val = algo_to_van[algo_name].get(metric)\n",
    "                        # if vanilla_val is None:\n",
    "                        #     continue\n",
    "                        \n",
    "                        # vanilla_val = np.mean(vanilla_val)\n",
    "\n",
    "                        vals = res.get(key)\n",
    "                        if vals:\n",
    "                            meanval = np.mean(vals)\n",
    "                            if standard_val:\n",
    "                                standard_val = np.mean(standard_val)\n",
    "                               \n",
    "                                old_add_inc_key = 'increase_from_baseline_{}'.format(key)\n",
    "                                new_add_inc_key = 'increase_{}'.format(key)\n",
    "                                add_inc = res.get(old_add_inc_key)\n",
    "                                if add_inc:\n",
    "                                    print('!')\n",
    "                                    input()\n",
    "                                add_inc_computed = meanval - standard_val\n",
    "                                per_inc_key = 'percent_increase_{}'.format(key)\n",
    "                                per_inc_computed = 100 * (meanval - standard_val) / standard_val\n",
    "                                uid_to_metric[uid][new_add_inc_key] = add_inc_computed\n",
    "                                uid_to_metric[uid][per_inc_key] = per_inc_computed\n",
    "\n",
    "\n",
    "                            # add_inc_vanilla = meanval - vanilla_val\n",
    "                            # per_inc_vanilla = 100 * (meanval - vanilla_val) / vanilla_val\n",
    "                            # uid_to_metric[uid]['vanilla' + new_add_inc_key] = add_inc_vanilla\n",
    "                            # uid_to_metric[uid]['vanilla' + per_inc_key] = per_inc_vanilla\n",
    "                            uid_to_metric[uid]['userfrac'] = userfrac\n",
    "                            uid_to_metric[uid]['ratingfrac'] = ratingfrac\n",
    "                            uid_to_metric[uid]['type'] = experiment_type\n",
    "                            if 'indices' in outname:\n",
    "                                uid_to_metric[uid]['indices'] = indices\n",
    "                        \n",
    "        as_df = pd.DataFrame.from_dict(uid_to_metric, orient='index')\n",
    "        \n",
    "        cols = list(as_df.columns.values)\n",
    "        for col in [\n",
    "            'userfrac', 'ratingfrac', 'indices', 'name', 'algo_name', 'within_run_identifier', 'name'\n",
    "        ]:\n",
    "            if col in cols:\n",
    "                cols.insert(0, cols.pop(cols.index(col)))\n",
    "        as_df[cols].to_csv(outname.replace('results/', 'processed_results/'))\n",
    "\n",
    "                            \n",
    "\n",
    "def parse():\n",
    "    \"\"\"\n",
    "    Parse args and handles list splitting\n",
    "\n",
    "    Example:\n",
    "    python process_results --sample_sizes 4,5,6,7 --num_samples 100\n",
    "    python process_results --grouping gender --userfracs 0.5,1 --ratingfracs 0.5,1\n",
    "    \"\"\"\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--grouping', default='sample_users')\n",
    "    parser.add_argument('--sample_sizes')\n",
    "    parser.add_argument('--num_samples', type=int)\n",
    "    parser.add_argument('--dataset', default='ml-1m')\n",
    "    parser.add_argument('--userfracs')\n",
    "    parser.add_argument('--ratingfracs')\n",
    "    parser.add_argument('--outnames')\n",
    "    parser.add_argument('--load_standards_from_json')\n",
    "    args = parser.parse_args()\n",
    "    if args.sample_sizes:\n",
    "        args.sample_sizes = [int(x) for x in args.sample_sizes.split(',')]\n",
    "        if args.num_samples is None:\n",
    "            args.num_samples = 1000\n",
    "    else:\n",
    "        args.sample_sizes = [None]\n",
    "\n",
    "    if args.userfracs:\n",
    "        args.userfracs = [float(x) for x in args.userfracs.split(',')]\n",
    "    else:\n",
    "        args.userfracs = [1.0]\n",
    "    if args.ratingfracs:\n",
    "        args.ratingfracs = [float(x) for x in args.ratingfracs.split(',')]\n",
    "    else:\n",
    "        args.ratingfracs = [1.0]\n",
    "\n",
    "    print(args.outnames)\n",
    "    if args.outnames:\n",
    "        args.outnames = args.outnames.split(',')\n",
    "\n",
    "    if args.grouping == 'sample':\n",
    "        args.grouping = 'sample_users'\n",
    "    main(args)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    parse()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
